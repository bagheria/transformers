{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dELWpyou789C"
   },
   "source": [
    "### Practical 3: Word Embedding\n",
    "#### Ayoub Bagheri\n",
    "<img src=\"img/uu_logo.png\" alt=\"logo\" align=\"right\" title=\"UU\" width=\"50\" height=\"20\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical we are going to apply different word embedding methods. For this purpose, we use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hX_spMIk9BQF"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np    \n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgw0Ld6kv9Ku"
   },
   "source": [
    "# First install the gensim library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0tEUKDfxJ5Y"
   },
   "source": [
    "In this practical session we're going to use the [gensim](https://radimrehurek.com/gensim/) library. This library offers a variety of methods to read\n",
    "in pre-trained word embeddings as well as train your own.\n",
    "\n",
    "The website contains a lot of documentation, for example here: https://radimrehurek.com/gensim/auto_examples/index.html#documentation\n",
    "\n",
    "If gensim isn't installed yet, you can use the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jUHdt_DtvrA3"
   },
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zfk4o1Cebn-J"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKUfWy0nwG0U"
   },
   "source": [
    "# Reading in a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79Mb6_c9wwvt"
   },
   "source": [
    "1\\. **Use the code below to load in a pre-trained GloVe model. Note: this can take around five minutes.**\n",
    "\n",
    "See https://github.com/RaRe-Technologies/gensim-data for an overview of the models you can try. For example\n",
    "\n",
    "*   word2vec-google-news-300: word2vec trained on Google news. 1662 MB.\n",
    "*   glove-twitter-200: trained on Twitter: 758 MB \n",
    "\n",
    "We're going to start with `glove-wiki-gigaword-300` which\n",
    "is 376.1MB to download. These embeddings are trained on \n",
    "Wikipedia (2014) and the Gigaword corpus, a large collection\n",
    "of newswire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xs4LeLkav_xy",
    "outputId": "7d30a1ed-e861-4856-db7e-7642bc911335"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91pmPOTX4bmE"
   },
   "source": [
    "# Exploring the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luAJI20EZfVM"
   },
   "source": [
    "2\\. **How many words does the vocabulary contain?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCYQx2rzamAe"
   },
   "source": [
    "3\\. **Is '*utrecht*' in the vocabulary?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWqjHy7JaCAq"
   },
   "source": [
    "4\\. **Print a word embedding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hYuk_2KaLO-"
   },
   "source": [
    "5\\. **How many dimensions does this embedding have?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02IVqpksawKL"
   },
   "source": [
    "6\\. **Explore the embeddings for a few other words. Can you find words that are *not* in the vocabulary?**\n",
    "\n",
    "(For example, think of uncommon words, misspellings, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omCh8m2k1kMd"
   },
   "source": [
    "# Vector arithmethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmIVSe-c1pOA"
   },
   "source": [
    "7\\. **Use the code below to calculate the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between two words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1vVWsuO1m9G",
    "outputId": "0f9aa8cc-03f6-4a51-af48-66dc6eefa4fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5970514"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity('university', 'student')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB1u026w1y4y"
   },
   "source": [
    "*Note*: cosine similarity is the same as the dot product between the normalized word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-UHTcX229Qn",
    "outputId": "3b00d278-f129-4826-a1c0-273a03e6f74c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5970514"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_university_norm = wv['university']/ LA.norm(wv['university'], 2)\n",
    "wv_student_norm = wv['student'] / LA.norm(wv['student'], 2)\n",
    "\n",
    "wv_university_norm.dot(wv_student_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2qWG2llbX6Q"
   },
   "source": [
    "# Similarity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSFauHoA4xF9"
   },
   "source": [
    "8\\. **Print the top 5 most similar words to `car`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YljWEl6W44jN"
   },
   "source": [
    "**Question**: What are the top 5 most similar words to *cat*?  And to *king*? And to *fast*? What kind of words often appear in the top? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVBB1MoOczHp"
   },
   "source": [
    "Now calculate the similarities between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYdtGXBxcyh-",
    "outputId": "90687581-9f70-4c40-ae62-4884c77c2dd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77922326"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity('buy', 'purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5VVQhkbdMo3",
    "outputId": "829ea172-699a-4f2c-a883-af4b5b3e126f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68167466"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity('cat', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHpGmyCwdP_T",
    "outputId": "227f7855-81c6-468a-a2ef-3623f94d365f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25130013"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity('car', 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTy98KEIdn1S"
   },
   "source": [
    "We can calculate the cosine similarity between a list of word pairs and correlate these with human ratings. One such dataset with human ratings is called WordSim353.\n",
    "\n",
    "**Goto** https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/wordsim353.tsv to get a sense of the data. \n",
    "\n",
    "\n",
    "Gensim already implements a method to evaluate a word embedding model using this data. \n",
    "* It calculates the cosine similarity between each word pair\n",
    "* It calculates both the Spearman and Pearson correlation coefficient between the cosine similarities and human judgements\n",
    "\n",
    "See https://radimrehurek.com/gensim/models/keyedvectors.html for a description of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3hbA_Pd5m2Y"
   },
   "source": [
    "# Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wpOgoulf_mh"
   },
   "source": [
    "Man is to woman as king is to. ..?\n",
    "\n",
    "This can be converted into vector arithmethics:\n",
    "\n",
    "```\n",
    "king - ? = man - woman.\n",
    "\n",
    "king - man + woman = ?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(negative=['man'], positive=['king', 'woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QpczXwlgUGW"
   },
   "source": [
    "france - paris + amsterdam = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(negative=['paris'], positive=['france', 'amsterdam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_qTUAGjgbdP"
   },
   "source": [
    "Note that it we would just retrieve the most similar words to '*amsterdam*' we would receive a different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDr8H4pp51ER",
    "outputId": "3d4dce72-1ca3-44f5-a764-6dd141702e7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rotterdam', 0.6485881209373474), ('schiphol', 0.5740087032318115), ('utrecht', 0.5608800053596497), ('netherlands', 0.5472348928451538), ('frankfurt', 0.5457332730293274)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['amsterdam'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plZLOUP2giIk"
   },
   "source": [
    "cat is to cats as girl is to ?\n",
    "\n",
    "```\n",
    "girl - ? = cat - cats\n",
    "girl - cat + cats = ?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(negative=['cat'], positive=['cats', 'girl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fsH7_DEh8Zt"
   },
   "source": [
    "Compare against a baseline. What if we would just have retrieved the most similar words to '*girl*'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['girl'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdTPXa2M8svY"
   },
   "source": [
    "**Fun**: Try a few of your own analogies, do you get the expected answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z_2LyUAiHm8"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDnfe0x_9izp"
   },
   "source": [
    "9\\. **We can't visualize embeddings in their raw format, because of their high dimensionality. However, we can use dimensionality reduction techniques such as PCA to project them onto a 2D space. Use the code below to do this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2ikcukvb8r8w"
   },
   "outputs": [],
   "source": [
    "def display_scatterplot(wv, words=None, sample=0):\n",
    "        \n",
    "    # first get the word vectors\n",
    "    word_vectors = np.array([wv[w] for w in words])\n",
    "\n",
    "    # transform the data using PCA\n",
    "    wv_PCA = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    plt.scatter(wv_PCA[:,0], wv_PCA[:,1], \n",
    "                edgecolors='k', c='r')\n",
    "    \n",
    "    for word, (x,y) in zip(words, wv_PCA):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scatterplot(wv, \n",
    "                        ['dog', 'cat', 'dogs', 'cats', 'horse', 'tiger',\n",
    "                         'university', 'lesson', 'student', 'students',\n",
    "                         'netherlands', 'amsterdam', 'utrecht', 'belgium', 'spain', 'china',\n",
    "                         'coffee', 'tea', 'pizza', 'sushi', 'sandwich',\n",
    "                         'car', 'train', 'bike', 'bicycle', 'trains'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOVm0bj8AKtY"
   },
   "source": [
    "**Question**: What do you notice in this plot? Do the distances between the words make sense? Any surprises? Feel free to add your own words!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word_embeddings_summerschool.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
