{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 1: Pre-processing\n",
    "#### Ayoub Bagheri\n",
    "<img src=\"img/uu_logo.png\" alt=\"logo\" align=\"right\" title=\"UU\" width=\"50\" height=\"20\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we are going to do some text preprocessing! Are you looking for Python documentation to refresh you knowledge of programming? If so, you can check https://docs.python.org/3/reference/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser, with:\n",
    "* Zero configuration required\n",
    "* Free access to GPUs\n",
    "* Easy sharing\n",
    "\n",
    "Colab notebooks are Jupyter notebooks that are hosted by Colab. Here you can find links to more detailed introductions to Colab: https://colab.research.google.com/notebooks/intro.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd \n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # for bag of words feature extraction\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use can simply run `!pip install package_name` to install a package. Generally, you only need to install each package once on your computer and load it again, however, in Colab you may need to reinstall a package once you are reconnecting to the network.\n",
    "\n",
    "NB: nltk package comes with many corpora, toy grammars, trained models, etc. A complete list is posted at: http://nltk.org/nltk_data/\n",
    "\n",
    "To install the data, after installing nltk, use nltk’s data downloader as \"nltk.download()\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing a simple text\n",
    "#### `If you feel comfortable with Python and Google colab skip to question 8.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Colab and create a new empty notebook to work with Python 3! Go to https://colab.research.google.com/ and login with your account. Then click on \"File $\\rightarrow$ New notebook\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. **Text is known as a string object or as an array of characters. Create an object _a_ with the value of \"Hello @Text Mining World! I'm here to learn everything, right?\", and then print it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. **Since this is an array, print the first and last character of your object.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. **Use the function lower() from the nltk package to convert the characters in the object to their lowercase form and save it into a new object b.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. **Use the _string_ package to print the list of punctuations.**\n",
    "\n",
    "Punctuations can separate characters, words, phrases, or sentences. In some applications they are very important to the task at hand, in others they are redundant and should be removed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. **Use the code below to remove the punctuations from the our string. Name your object c.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remmebr there are many ways to remove punctuations! This is only one of them:\n",
    "c = \"\".join([char for char in b if char not in string.punctuation])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. **Use the `word_tokenize` function from _nltk_ and tokenize string *b*. Compare that with the tokenization of string _c_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. **Use sent_tokenize function from the _nltk_ package and split string b into sentences. Compare that with the sentence tokenization of string c.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing a text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing a corpus is similar to pre-processing a text string. \n",
    "\n",
    "Here are some resources for puclic text data sets:\n",
    "- CLARIN Resource Families: https://www.clarin.eu/portal\n",
    "- UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table\n",
    "- Kaggle: https://www.kaggle.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to analyze and pre-process the `Taylor Swift song lyrics` from all her albums. We downloaded this data set from the Kaggle website and put that already in the data folder. Here, you can find more information about the original data set: https://www.kaggle.com/PromptCloudHQ/taylor-swift-song-lyrics-from-all-the-albums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. **Read the “taylor_swift.csv” data set. Check the head and tail functions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. **Use the code below to replace the '\\n' notations with a space character to remove the line breaks. In this code, a new column has been added to the dataframe, _Preprocessed_ _Lyrics_. We are going to fill this column out with the preprocessed text including the steps in the following questions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_linebreaks(text):\n",
    "    \"\"\"custom function to remove the line breaks\"\"\"\n",
    "    return re.sub(r'\\n', ' ', text)\n",
    "\n",
    "ts_lyrics[\"Preprocessed Lyrics\"] = ts_lyrics[\"Lyrics\"].apply(lambda text: remove_linebreaks(text))\n",
    "ts_lyrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. **Write a custom function to remove the punctuations. (Hint: You can use the method in question 5 or make use of the function maketrans from the string package.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. **Convert the characters to their lower forms. Think about why and when we need this step in our analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. **Use the code below to list the 20 most frequent terms in your preprocessed lyrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get all lyrics in one text, you can concatenate all of them using the \" \".join(list) syntax, \n",
    "# which joins all elements in a list separating them by whitespace.\n",
    "text = \" \".join(lyric for lyric in ts_lyrics[\"Preprocessed Lyrics\"])\n",
    "\n",
    "# split() returns list of all the words in the string\n",
    "split_it = text.split()\n",
    "  \n",
    "# Pass the split_it list to instance of Counter class.\n",
    "Counter = Counter(split_it)\n",
    "  \n",
    "# most_common() produces k frequently encountered input values and their respective counts.\n",
    "most_occur = Counter.most_common(20)\n",
    "  \n",
    "print(most_occur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that these are mainly the stop words. Before removing them let's plot a worcloud of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13\\. **Use the code below to plot a wordcloud with a maximum of 50 words. Check the command _?WordCloud_ to review the help page of this function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\").generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14\\. **Run the codes given below to remvoe the stop words, and update the stop words by adding words: \"im\", \"youre\", \"id\", \"dont\", \"cant\", \"didnt\", \"ive\", \"ill\", \"hasnt\". Show the 20 most frequent terms and plot the wordcould of 50 words again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the code nltk.download('stopwords') if needed\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.update([\"im\", \"youre\", \"id\", \"dont\", \"cant\", \"didnt\", \"ive\", \"ill\", \"hasnt\"])\n",
    "# stop_words.discard('word') # this is when you want to remove a word from the list\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "ts_lyrics[\"Preprocessed Lyrics\"] = ts_lyrics[\"Preprocessed Lyrics\"].apply(lambda text: remove_stopwords(text))\n",
    "ts_lyrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# To get all lyrics in one text, you can concatenate all of them using the \" \".join(list) syntax, \n",
    "# which joins all elements in a list separating them by whitespace.\n",
    "text = \" \".join(lyric for lyric in ts_lyrics[\"Preprocessed Lyrics\"])\n",
    "\n",
    "# split() returns list of all the words in the string\n",
    "split_it = text.split()\n",
    "  \n",
    "# Pass the split_it list to instance of Counter class.\n",
    "Counter = Counter(split_it)\n",
    "  \n",
    "# most_common() produces k frequently encountered input values and their respective counts.\n",
    "most_occur = Counter.most_common(20)\n",
    "  \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15\\. **We can apply stemming or lemmatization on our text data. Apply a lemmatizer from nltk and save the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector space model: Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16\\. **Use the CountVectorizer from the sklearn package and build a bag of words model on _Preprocessed Lyrics_ based on term frequency. Check the shape of the output matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17\\. **Inspect the first 100 terms in the vocabulary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18\\. **Using TfidfVectorizer, you can create a model based on tfidf. Use the code below to apply a TfidfVectorizer on your text data. Does the shape of the output matrix differ from dtm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19\\. **Use the TfidfVectorizer to create an n-gram based model with n = 1 and 2. (Hint: Use the ngram_range argument to determine the lower and upper boundary of the range of n-values for different n-grams to be extracted.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20\\. **We want to compare the lyrics of Friends theme song with the lyrics of Taylor Swift's songs and find the most similar one. Use the code below to, first, apply the pre-processing steps and then transform the text into counts and tfidf vectors. Do the bag of words models (tf vs tfidf) agree on the most similar song to Friends theme song?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_theme_lyrics = \"So no one told you life was going to be this way. Your job's a joke, you're broke, you're love life's DOA. It's like you're always stuck in second gear, When it hasn\\'t been your day, your week, your month, or even your year. But, I\\'ll be there for you, when the rain starts to pour. I\\'ll be there for you, like I\\'ve been there before. I\\'ll be there for you, cause you\\'re there for me too.\"\n",
    "friends_theme_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_theme_lyrics = remove_punctuation(friends_theme_lyrics)\n",
    "friends_theme_lyrics = friends_theme_lyrics.lower()\n",
    "friends_theme_lyrics = remove_stopwords(friends_theme_lyrics)\n",
    "friends_theme_lyrics = lemmatize_words(friends_theme_lyrics)\n",
    "friends_theme_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_theme_lyrics_tf = vectorizer1.transform([friends_theme_lyrics])\n",
    "friends_theme_lyrics_tf.shape\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print the cosine similarity matrix\n",
    "cosine_sim_dtm = cosine_similarity(dtm, friends_theme_lyrics_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = np.argmax(cosine_sim_dtm, axis=0)\n",
    "print(cosine_sim_dtm[max_index])\n",
    "max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_lyrics.iloc[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_lyrics[\"Preprocessed Lyrics\"].iloc[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_theme_lyrics_tfidf = vectorizer3.transform([friends_theme_lyrics])\n",
    "print(friends_theme_lyrics_tfidf.shape)\n",
    "print(tfidf_matrix3.shape)\n",
    "# compute and print the cosine similarity matrix\n",
    "cosine_sim_tfidf = cosine_similarity(tfidf_matrix3, friends_theme_lyrics_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = np.argmax(cosine_sim_tfidf, axis=0)\n",
    "print(cosine_sim_tfidf[max_index])\n",
    "max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_lyrics.iloc[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_lyrics[\"Preprocessed Lyrics\"].iloc[16]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
